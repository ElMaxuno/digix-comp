{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from category_encoders.count import CountEncoder\n",
    "import gc\n",
    "\n",
    "NUM_FOLDS=5\n",
    "RANDOM_STATE = 800\n",
    "\n",
    "\n",
    "#     ░░░░░░░░░░░░        ▒▒▓▓██▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓▓    ░░░░░░░░░░░░░░        \n",
    "#           ░░░░░░░░░░░░  ██████░░░░░░░░░░░░░░░░  ░░░░░░████░░░░░░░░░░░░░░        \n",
    "#           ░░░░░░░░░░████  ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░████░░░░░░░░          \n",
    "#                   ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ██                \n",
    "#               ░░██    ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██              \n",
    "#             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██          \n",
    "#               ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██            \n",
    "#             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██          \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓        \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██        \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██        \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  ░░    ░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██      \n",
    "#       ▓▓░░░░░░████░░▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓░░░░░░░░░░▒▒▓▓    \n",
    "#       ██░░▒▒▓▓▒▒▒▒██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██▒▒░░██▒▒▒▒▒▒▒▒▒▒██░░░░░░░░░░░░░░▓▓░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██░░░░▒▒██▒▒▒▒▒▒██░░░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░██████▒▒░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░░░██░░██░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░░░░░░░░░░░██░░░░░░░░░░██░░░░░░░░░░░░░░░░▒▒▓▓░░██░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░▒▒░░░░░░░░██░░░░░░░░░░██░░░░░░░░░░░░░░░░██░░░░██░░░░░░░░░░██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▓▓▓▓▓▓▓▓░░▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░██▓▓▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▓▓▒▒▒▒██▓▓░░░░░░▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▓▓▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒██░░████████░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒████▒▒▒▒████████████░░░░░░░░░░░░░░░░░░░░████████████▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██  ██▒▒▒▒██  ████████░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██  ████████░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▓▓  ░░██░░░░░░░░░░░░░░░░░░░░██░░░░██▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▒▒████░░░░░░░░░░░░░░░░░░░░░░░░▓▓▓▓▒▒▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▒▒▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▓▓██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒██████░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░██▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒██▒▒▒▒██████░░░░░░░░░░░░░░░░░░░░██████▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████▒▒▒▒▒▒▒▒██░░░░████████████████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒██░░░░░░██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████████░░░░░░██████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▓▓░░    \n",
    "#       ░░██▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▓▓▓▓▓▓▓▓▓▓██░░░░░░██▓▓▓▓▓▓▓▓▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_downsample(df, target, neg_frac = .1):\n",
    "    # Negative downsampling\n",
    "    df_pos = df[df[target]==1]\n",
    "    df_neg = df[df[target]==0]\n",
    "    \n",
    "    df_neg = df_neg.sample(int(df.shape[0] * neg_frac), random_state=42)\n",
    "    return pd.concat([df_pos,df_neg]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'label': 'int8',\n",
    " 'uid': 'int32',\n",
    " 'task_id': 'int16',\n",
    " 'adv_id': 'int16',\n",
    " 'creat_type_cd': 'int8',\n",
    " 'adv_prim_id': 'int16',\n",
    " 'dev_id': 'int8',\n",
    " 'inter_type_cd': 'int8',\n",
    " 'slot_id': 'int8',\n",
    " 'spread_app_id': 'int8',\n",
    " 'tags': 'int8',\n",
    " 'app_first_class': 'int8',\n",
    " 'app_second_class': 'int8',\n",
    " 'age': 'int8',\n",
    " 'city': 'int16',\n",
    " 'city_rank': 'int8',\n",
    " 'device_name': 'int8',\n",
    " 'device_size': 'int16',\n",
    " 'career': 'int8',\n",
    " 'gender': 'int8',\n",
    " 'net_type': 'int8',\n",
    " 'residence': 'int8',\n",
    " 'his_app_size': 'int8',\n",
    " 'his_on_shelf_time': 'int8',\n",
    " 'app_score': 'int8',\n",
    " 'emui_dev': 'int8',\n",
    " 'list_time': 'int8',\n",
    " 'device_price': 'int8',\n",
    " 'up_life_duration': 'int8',\n",
    " 'up_membership_grade': 'int8',\n",
    " 'membership_life_duration': 'int8',\n",
    " 'consume_purchase': 'int8',\n",
    " 'communication_onlinerate': 'object',\n",
    " 'communication_avgonline_30d': 'int8',\n",
    " 'indu_name': 'int8',\n",
    " 'pt_d': 'int8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = pd.read_csv('train_data/train_data.csv', sep=\"|\",dtype = dtypes ).reset_index()\n",
    "test_df = pd.read_csv('test_data_A.csv', sep=\"|\",dtype = dtypes ).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "b = pd.concat([train_df.drop('label',axis=1),test_df.drop('id',axis=1)]).sort_values(['pt_d','index']).reset_index()\n",
    "for col in ['task_id','adv_prim_id']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a = b[b.pt_d==i-1].groupby(['uid',col]).size().reset_index().rename({0:f'uid_{col}_ptd'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "for col in ['task_id','adv_id','slot_id','spread_app_id']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a = b[b.pt_d.isin(range(i))].groupby(['uid',col]).size().reset_index().rename({0:f'uid_{col}_ptd_rng'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "b = train_df.sort_values(['pt_d','index']).reset_index()\n",
    "\n",
    "\n",
    "for col in ['uid','task_id','adv_id']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a =  b[b.pt_d==i-1][[col,'label']].groupby([col]).mean().reset_index().rename({'label':f'{col}_targ_mean'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in ['uid','task_id']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a =  b[b.pt_d.isin(range(i))][[col,'label']].groupby([col]).mean().reset_index().rename({'label':f'{col}_targ_mean_sumed'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cnt = CountEncoder(cols=['uid','task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','indu_name'])\n",
    "\n",
    "cnt.fit(pd.concat([train_df.drop('label',axis =1),test_df.drop('id',axis=1)]))\n",
    "\n",
    "train_m = cnt.transform(train_df.drop('label',axis=1))\n",
    "test_m = cnt.transform(test_df.drop('id',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m['label'] = train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS = ['uid','task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','indu_name','age','uid_task_id_ptd','uid_adv_prim_id_ptd',\n",
    "        'uid_task_id_ptd','uid_adv_id_ptd_rng','uid_slot_id_ptd_rng','uid_spread_app_id_ptd_rng','his_app_size','app_first_class','gender',\n",
    "        'career','device_name','uid_targ_mean','task_id_targ_mean','adv_id_targ_mean','uid_targ_mean_sumed',\n",
    "        'task_id_targ_mean_sumed']\n",
    "\n",
    "PARAMS = {  'learning_rate':0.05, 'max_bin': 100, 'max_depth': -1, 'min_child_samples': 5000,\n",
    "        'n_estimators': 200,\n",
    "        'num_leaves': 250,'objective': 'binary', 'n_jobs': -1,'subsample':.7,'boost_from_average': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6277359\n",
       "Name: pt_d, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_m[:6277359].pt_d.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    6009176\n",
       "Name: pt_d, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_m[35897957:].pt_d.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8296922255901467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_downded = negative_downsample(train_m[6277359:35897957],'label')\n",
    "train_data = lgb.Dataset(train_downded.drop('label',axis=1)[FEATS], label=train_downded.label, free_raw_data=True)\n",
    "aa = lgb.train(PARAMS,train_data)\n",
    "preds = aa.predict(train_m[35897957:].drop('label',axis=1)[FEATS])\n",
    "roc_auc_score(train_m[35897957:].label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "train_downded = negative_downsample(train_m[6277359:],'label')\n",
    "train_data = lgb.Dataset(train_downded.drop('label',axis=1)[FEATS], label=train_downded.label, free_raw_data=True)\n",
    "aa = lgb.train(PARAMS,train_data)\n",
    "preds = aa.predict(test_m[FEATS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['probability']= preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id','probability']].set_index('id').to_csv('0.8296922255901467.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
