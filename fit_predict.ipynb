{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from category_encoders.count import CountEncoder\n",
    "import gc\n",
    "\n",
    "NUM_FOLDS=5\n",
    "RANDOM_STATE = 800\n",
    "\n",
    "\n",
    "#     ░░░░░░░░░░░░        ▒▒▓▓██▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓▓    ░░░░░░░░░░░░░░        \n",
    "#           ░░░░░░░░░░░░  ██████░░░░░░░░░░░░░░░░  ░░░░░░████░░░░░░░░░░░░░░        \n",
    "#           ░░░░░░░░░░████  ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░████░░░░░░░░          \n",
    "#                   ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ██                \n",
    "#               ░░██    ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██              \n",
    "#             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██          \n",
    "#               ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██            \n",
    "#             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██          \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓        \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██        \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██        \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  ░░    ░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██      \n",
    "#       ▓▓░░░░░░████░░▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓░░░░░░░░░░▒▒▓▓    \n",
    "#       ██░░▒▒▓▓▒▒▒▒██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██▒▒░░██▒▒▒▒▒▒▒▒▒▒██░░░░░░░░░░░░░░▓▓░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██░░░░▒▒██▒▒▒▒▒▒██░░░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░██████▒▒░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░░░██░░██░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░░░░░░░░░░░██░░░░░░░░░░██░░░░░░░░░░░░░░░░▒▒▓▓░░██░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░▒▒░░░░░░░░██░░░░░░░░░░██░░░░░░░░░░░░░░░░██░░░░██░░░░░░░░░░██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▓▓▓▓▓▓▓▓░░▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░██▓▓▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▓▓▒▒▒▒██▓▓░░░░░░▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▓▓▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒██░░████████░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒████▒▒▒▒████████████░░░░░░░░░░░░░░░░░░░░████████████▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██  ██▒▒▒▒██  ████████░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██  ████████░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▓▓  ░░██░░░░░░░░░░░░░░░░░░░░██░░░░██▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▒▒████░░░░░░░░░░░░░░░░░░░░░░░░▓▓▓▓▒▒▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▒▒▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▓▓██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒██████░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░██▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒██▒▒▒▒██████░░░░░░░░░░░░░░░░░░░░██████▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████▒▒▒▒▒▒▒▒██░░░░████████████████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒██░░░░░░██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████████░░░░░░██████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▓▓░░    \n",
    "#       ░░██▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▓▓▓▓▓▓▓▓▓▓██░░░░░░██▓▓▓▓▓▓▓▓▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ░░░░░░░░░░░░        ▒▒▓▓██▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓▓    ░░░░░░░░░░░░░░        \n",
    "#           ░░░░░░░░░░░░  ██████░░░░░░░░░░░░░░░░  ░░░░░░████░░░░░░░░░░░░░░        \n",
    "#           ░░░░░░░░░░████  ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░████░░░░░░░░          \n",
    "#                   ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ██                \n",
    "#               ░░██    ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██              \n",
    "#             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██          \n",
    "#               ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██            \n",
    "#             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██          \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓        \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██        \n",
    "#           ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██        \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  ░░    ░░██      \n",
    "#         ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██      \n",
    "#       ▓▓░░░░░░████░░▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓░░░░░░░░░░▒▒▓▓    \n",
    "#       ██░░▒▒▓▓▒▒▒▒██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██▒▒░░██▒▒▒▒▒▒▒▒▒▒██░░░░░░░░░░░░░░▓▓░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██░░░░▒▒██▒▒▒▒▒▒██░░░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░░░██░░░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░██████▒▒░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░░░██░░██░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░░░░░░░░░░░██░░░░░░░░░░██░░░░░░░░░░░░░░░░▒▒▓▓░░██░░░░░░░░░░██    \n",
    "#       ██░░░░░░░░▒▒░░░░░░░░██░░░░░░░░░░██░░░░░░░░░░░░░░░░██░░░░██░░░░░░░░░░██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▓▓▓▓▓▓▓▓░░▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░██▓▓▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▓▓▒▒▒▒██▓▓░░░░░░▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▓▓▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒██░░░█████░░░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒████▒▒▒▒█████░░░░░██░░░░░░░░░░░░░░░░░░░░███░░░░░░███▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██  ██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░████████  ██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▓▓  ░░░░░░░░░░░░░░░░░░░░░░░░██░░░░██▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▓▓▒▒▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒██░░██▒▒▒▒██▒▒▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▒▒▒▒██▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▓▓██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▓▓▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒██████░░░░░░░░░░░░░░██░░░░░░░░░░░░░░░░██▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒██▒▒▒▒██████░░░░░░░░░░░░░░░░░░░░██████▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████▒▒▒▒▒▒▒▒██░░░░████████████████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒██░░░░░░██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n",
    "#       ██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████████░░░░░░██████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▓▓░░    \n",
    "#       ░░██▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▓▓▓▓▓▓▓▓▓▓██░░░░░░██▓▓▓▓▓▓▓▓▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-10-5096b85cee69>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-5096b85cee69>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ░\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "░"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-11-cfac5b8ac397>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-cfac5b8ac397>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    █\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "█"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_downsample(df, target, neg_frac = .1):\n",
    "    # Negative downsampling\n",
    "    df_pos = df[df[target]==1]\n",
    "    df_neg = df[df[target]==0]\n",
    "    \n",
    "    df_neg = df_neg.sample(int(df.shape[0] * neg_frac), random_state=42)\n",
    "    return pd.concat([df_pos,df_neg]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'label': 'int8',\n",
    " 'uid': 'int32',\n",
    " 'task_id': 'int16',\n",
    " 'adv_id': 'int16',\n",
    " 'creat_type_cd': 'int8',\n",
    " 'adv_prim_id': 'int16',\n",
    " 'dev_id': 'int8',\n",
    " 'inter_type_cd': 'int8',\n",
    " 'slot_id': 'int8',\n",
    " 'spread_app_id': 'int8',\n",
    " 'tags': 'int8',\n",
    " 'app_first_class': 'int8',\n",
    " 'app_second_class': 'int8',\n",
    " 'age': 'int8',\n",
    " 'city': 'int16',\n",
    " 'city_rank': 'int8',\n",
    " 'device_name': 'int8',\n",
    " 'device_size': 'int16',\n",
    " 'career': 'int8',\n",
    " 'gender': 'int8',\n",
    " 'net_type': 'int8',\n",
    " 'residence': 'int8',\n",
    " 'his_app_size': 'int8',\n",
    " 'his_on_shelf_time': 'int8',\n",
    " 'app_score': 'int8',\n",
    " 'emui_dev': 'int8',\n",
    " 'list_time': 'int8',\n",
    " 'device_price': 'int8',\n",
    " 'up_life_duration': 'int8',\n",
    " 'up_membership_grade': 'int8',\n",
    " 'membership_life_duration': 'int8',\n",
    " 'consume_purchase': 'int8',\n",
    " 'communication_onlinerate': 'object',\n",
    " 'communication_avgonline_30d': 'int8',\n",
    " 'indu_name': 'int8',\n",
    " 'pt_d': 'int8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 5315.45 Mb (14.2% reduction)\n",
      "Mem. usage decreased to 129.70 Mb (13.9% reduction)\n",
      "Mem. usage decreased to 5395.38 Mb (4.3% reduction)\n",
      "Mem. usage decreased to 131.61 Mb (4.2% reduction)\n",
      "Mem. usage decreased to 5475.31 Mb (4.2% reduction)\n",
      "Mem. usage decreased to 133.51 Mb (4.1% reduction)\n",
      "Mem. usage decreased to 5555.24 Mb (4.1% reduction)\n",
      "Mem. usage decreased to 135.42 Mb (4.1% reduction)\n",
      "Mem. usage decreased to 5635.17 Mb (4.1% reduction)\n",
      "Mem. usage decreased to 137.33 Mb (4.0% reduction)\n",
      "Mem. usage decreased to 5715.10 Mb (4.0% reduction)\n",
      "Mem. usage decreased to 139.24 Mb (3.9% reduction)\n",
      "Mem. usage decreased to 5795.03 Mb (4.0% reduction)\n",
      "Mem. usage decreased to 141.14 Mb (3.9% reduction)\n",
      "Mem. usage decreased to 5874.97 Mb (3.9% reduction)\n",
      "Mem. usage decreased to 143.05 Mb (3.8% reduction)\n",
      "Mem. usage decreased to 5954.90 Mb (3.9% reduction)\n",
      "Mem. usage decreased to 144.96 Mb (3.8% reduction)\n",
      "Mem. usage decreased to 6034.83 Mb (3.8% reduction)\n",
      "Mem. usage decreased to 146.87 Mb (3.7% reduction)\n",
      "Mem. usage decreased to 6114.76 Mb (3.8% reduction)\n",
      "Mem. usage decreased to 148.77 Mb (3.7% reduction)\n",
      "Mem. usage decreased to 6194.69 Mb (3.7% reduction)\n",
      "Mem. usage decreased to 150.68 Mb (3.7% reduction)\n",
      "Mem. usage decreased to 6274.62 Mb (3.7% reduction)\n",
      "Mem. usage decreased to 152.59 Mb (3.6% reduction)\n",
      "Mem. usage decreased to 6354.56 Mb (3.6% reduction)\n",
      "Mem. usage decreased to 154.50 Mb (3.6% reduction)\n",
      "Mem. usage decreased to 6434.49 Mb (3.6% reduction)\n",
      "Mem. usage decreased to 156.40 Mb (3.5% reduction)\n",
      "Mem. usage decreased to 6514.42 Mb (3.6% reduction)\n",
      "Mem. usage decreased to 158.31 Mb (3.5% reduction)\n",
      "Mem. usage decreased to 6594.35 Mb (3.5% reduction)\n",
      "Mem. usage decreased to 160.22 Mb (3.4% reduction)\n",
      "Mem. usage decreased to 6674.28 Mb (3.5% reduction)\n",
      "Mem. usage decreased to 162.12 Mb (3.4% reduction)\n",
      "Mem. usage decreased to 6754.21 Mb (3.4% reduction)\n",
      "Mem. usage decreased to 164.03 Mb (3.4% reduction)\n",
      "Mem. usage decreased to 6834.14 Mb (3.4% reduction)\n",
      "Mem. usage decreased to 165.94 Mb (3.3% reduction)\n",
      "Mem. usage decreased to 6914.08 Mb (3.4% reduction)\n",
      "Mem. usage decreased to 167.85 Mb (3.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "# train_df  = pd.read_csv('train_data/train_data.csv', sep=\"|\",dtype = dtypes ).reset_index()\n",
    "# test_df = pd.read_csv('test_data_A.csv', sep=\"|\",dtype = dtypes ).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# b = pd.concat([train_df.drop('label',axis=1),test_df.drop('id',axis=1)]).sort_values(['pt_d','index']).reset_index()\n",
    "# for col in ['task_id','adv_prim_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a = b[b.pt_d==i-1].groupby(['uid',col]).size().reset_index().rename({0:f'uid_{col}_ptd'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "# for col in ['task_id','adv_id','slot_id','spread_app_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a = b[b.pt_d.isin(range(i))].groupby(['uid',col]).size().reset_index().rename({0:f'uid_{col}_ptd_rng'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "# b = train_df.sort_values(['pt_d','index']).reset_index()\n",
    "\n",
    "\n",
    "# for col in ['uid','task_id','adv_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a =  b[b.pt_d==i-1][[col,'label']].groupby([col]).mean().reset_index().rename({'label':f'{col}_targ_mean'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for col in ['uid','task_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a =  b[b.pt_d.isin(range(i))][[col,'label']].groupby([col]).mean().reset_index().rename({'label':f'{col}_targ_mean_sumed'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "# for col in ['task_id','adv_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a =  b[b.pt_d.isin(range(i))][['uid',col,'label']].groupby(['uid',col]).mean().reset_index().rename({'label':f'uid_{col}_targ_mean_sumed'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=['uid',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "# for col in ['uid','task_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a =  b[b.pt_d.isin(range(i))][[col,'label']].groupby([col]).sum().reset_index().rename({'label':f'{col}_targ_sum_sumed'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "# for col in ['task_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a =  b[b.pt_d.isin(range(i))][['career','age','gender',col,'label']].groupby(['career','age','gender',col]).mean().reset_index().rename({'label':f'career_age_gender_{col}_targ_mean_sumed'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=['career','age','gender',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=['career','age','gender',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "# for col in ['uid','task_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a =  b[b.pt_d.isin(range(i))][[col,'label']].groupby([col]).std().reset_index().rename({'label':f'{col}_targ_mean_sumed'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for col in ['uid','task_id','adv_id','slot_id','spread_app_id']:\n",
    "#     conts = []\n",
    "#     for i in range(2,9):\n",
    "#         a = b[b.pt_d.isin(range(i))].groupby([col]).size().reset_index().rename({0:f'{col}_ptd_rng'},axis=1)\n",
    "#         a['pt_d'] = i\n",
    "\n",
    "#         conts.append(a)\n",
    "#     c = pd.concat(conts)\n",
    "#     train_df = pd.merge(train_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "#     test_df = pd.merge(test_df,c,how='left', on=[col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "#     gc.collect()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# groups  = pd.concat([train_df[['uid']],test_df[['uid']]]).groupby('uid').cumcount()\n",
    "\n",
    "# train_df['uid_cumcount'] = groups[:len(train_df)]\n",
    "# test_df['uid_cumcount'] = groups[len(train_df):]\n",
    "\n",
    "# groups  = pd.concat([train_df[['uid','task_id']],test_df[['uid','task_id']]]).groupby(['uid','task_id']).cumcount()\n",
    "# gc.collect()\n",
    "# train_df['uid_task_id_cumcount'] = groups[:len(train_df)]\n",
    "# test_df['uid_task_id_cumcount'] = groups[len(train_df):]\n",
    "\n",
    "# groups  = pd.concat([train_df[['uid','spread_app_id']],test_df[['uid','spread_app_id']]]).groupby(['uid','spread_app_id']).cumcount()\n",
    "# gc.collect()\n",
    "# train_df['uid_spread_app_id_cumcount'] = groups[:len(train_df)]\n",
    "# test_df['uid_spread_app_id_cumcount'] = groups[len(train_df):]\n",
    "\n",
    "\n",
    "# for col in ['task_id','spread_app_id']:\n",
    "#     groups  = pd.concat([train_df[[col]],test_df[[col]]]).groupby(col).cumcount()\n",
    "#     gc.collect()\n",
    "#     train_df[f'{col}_cumcount'] = groups[:len(train_df)]\n",
    "#     test_df[f'{col}_cumcount'] = groups[len(train_df):]\n",
    "\n",
    "# save_obj(test_df,'test_preproc')\n",
    "# save_obj(train_df,'train_preproc')\n",
    "\n",
    "train_df= load_obj('train_preproc.pkl').sort_values(['pt_d','index']).pipe(reduce_mem_usage)\n",
    "test_df = load_obj('test_preproc.pkl').sort_values(['pt_d','index']).pipe(reduce_mem_usage)\n",
    "b = train_df.sort_values(['pt_d','index']).reset_index()\n",
    "for col in ['task_id']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a =  b[b.pt_d.isin(range(i))][['uid_cumcount',col,'label']].groupby(['uid_cumcount',col]).mean().reset_index().rename({'label':f'uid_cumcount_{col}_targ_mean_sumed'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=['uid_cumcount',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=['uid_cumcount',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "for col in ['task_id']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a =  b[b.pt_d.isin(range(i))][['age',col,'label']].groupby(['age',col]).mean().reset_index().rename({'label':f'age_{col}_targ_mean_sumed'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=['age',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=['age',col,'pt_d']).sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "for col in [ 'city',\n",
    "       'city_rank', 'device_name', 'device_size', 'career', 'gender',\n",
    "        'communication_onlinerate',\n",
    "       'communication_avgonline_30d', 'indu_name']:\n",
    "    conts = []\n",
    "    for i in range(2,9):\n",
    "        a =  b[b.pt_d.isin(range(i))][[col,'task_id','label']].groupby([col,'task_id']).mean().reset_index().rename({'label':f'{col}_task_id_targ_mean_sumed'},axis=1)\n",
    "        a['pt_d'] = i\n",
    "        conts.append(a)\n",
    "    c = pd.concat(conts)\n",
    "    train_df = pd.merge(train_df,c,how='left', on=[col,'task_id','pt_d']).sort_values('index').reset_index(drop=True).pipe(reduce_mem_usage)\n",
    "    test_df = pd.merge(test_df,c,how='left', on=[col,'task_id','pt_d']).sort_values('index').reset_index(drop=True).pipe(reduce_mem_usage)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "cnt = CountEncoder(cols=['uid','task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','indu_name'])\n",
    "\n",
    "cnt.fit(pd.concat([train_df.drop('label',axis =1),test_df.drop(['id'],axis=1)]))\n",
    "\n",
    "train_m = cnt.transform(train_df.drop('label',axis=1))\n",
    "test_m = cnt.transform(test_df.drop('id',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'id', 'uid', 'task_id', 'adv_id', 'creat_type_cd',\n",
       "       'adv_prim_id', 'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id',\n",
       "       'tags', 'app_first_class', 'app_second_class', 'age', 'city',\n",
       "       'city_rank', 'device_name', 'device_size', 'career', 'gender',\n",
       "       'net_type', 'residence', 'his_app_size', 'his_on_shelf_time',\n",
       "       'app_score', 'emui_dev', 'list_time', 'device_price',\n",
       "       'up_life_duration', 'up_membership_grade', 'membership_life_duration',\n",
       "       'consume_purchase', 'communication_onlinerate',\n",
       "       'communication_avgonline_30d', 'indu_name', 'pt_d', 'uid_task_id_ptd',\n",
       "       'uid_adv_prim_id_ptd', 'uid_task_id_ptd_rng', 'uid_adv_id_ptd_rng',\n",
       "       'uid_slot_id_ptd_rng', 'uid_spread_app_id_ptd_rng', 'uid_targ_mean',\n",
       "       'task_id_targ_mean', 'adv_id_targ_mean', 'uid_targ_mean_sumed_x',\n",
       "       'task_id_targ_mean_sumed_x', 'uid_task_id_targ_mean_sumed',\n",
       "       'uid_adv_id_targ_mean_sumed', 'uid_targ_sum_sumed',\n",
       "       'task_id_targ_sum_sumed', 'career_age_gender_task_id_targ_mean_sumed',\n",
       "       'uid_targ_mean_sumed_y', 'task_id_targ_mean_sumed_y', 'uid_ptd_rng',\n",
       "       'task_id_ptd_rng', 'adv_id_ptd_rng', 'slot_id_ptd_rng',\n",
       "       'spread_app_id_ptd_rng', 'uid_cumcount', 'uid_task_id_cumcount',\n",
       "       'uid_spread_app_id_cumcount', 'task_id_cumcount',\n",
       "       'spread_app_id_cumcount', 'uid_cumcount_task_id_targ_mean_sumed',\n",
       "       'age_task_id_targ_mean_sumed', 'age_task_id_cumcount',\n",
       "       'city_task_id_targ_mean_sumed', 'city_rank_task_id_targ_mean_sumed',\n",
       "       'device_name_task_id_targ_mean_sumed',\n",
       "       'device_size_task_id_targ_mean_sumed', 'career_task_id_targ_mean_sumed',\n",
       "       'gender_task_id_targ_mean_sumed', 'net_type_task_id_targ_mean_sumed',\n",
       "       'residence_task_id_targ_mean_sumed',\n",
       "       'his_app_size_task_id_targ_mean_sumed',\n",
       "       'his_on_shelf_time_task_id_targ_mean_sumed',\n",
       "       'app_score_task_id_targ_mean_sumed', 'emui_dev_task_id_targ_mean_sumed',\n",
       "       'list_time_task_id_targ_mean_sumed',\n",
       "       'device_price_task_id_targ_mean_sumed',\n",
       "       'up_life_duration_task_id_targ_mean_sumed',\n",
       "       'up_membership_grade_task_id_targ_mean_sumed',\n",
       "       'membership_life_duration_task_id_targ_mean_sumed',\n",
       "       'consume_purchase_task_id_targ_mean_sumed',\n",
       "       'communication_onlinerate_task_id_targ_mean_sumed',\n",
       "       'communication_avgonline_30d_task_id_targ_mean_sumed',\n",
       "       'indu_name_task_id_targ_mean_sumed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m['label'] = train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    6009176\n",
       "Name: pt_d, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_m[35897957:].pt_d.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6277359\n",
       "Name: pt_d, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_m[:6277359].pt_d.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.834216807637092"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATS = ['uid','task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','indu_name','age','uid_task_id_ptd','uid_adv_prim_id_ptd',\n",
    "        'uid_task_id_ptd','uid_adv_id_ptd_rng','uid_slot_id_ptd_rng','uid_spread_app_id_ptd_rng','his_app_size','app_first_class','gender',\n",
    "        'career','device_name','uid_targ_mean','task_id_targ_mean','adv_id_targ_mean','uid_targ_mean_sumed_x',\n",
    "        'task_id_targ_mean_sumed_x','uid_task_id_targ_mean_sumed','uid_adv_id_targ_mean_sumed','uid_targ_sum_sumed','task_id_targ_sum_sumed'\n",
    "         ,'career_age_gender_task_id_targ_mean_sumed', 'uid_ptd_rng','app_first_class','consume_purchase','up_membership_grade','up_life_duration',\n",
    "        'list_time','uid_cumcount','uid_task_id_cumcount','uid_spread_app_id_cumcount','uid_cumcount_task_id_targ_mean_sumed','age_task_id_targ_mean_sumed',\n",
    "         'city_task_id_targ_mean_sumed', 'city_rank_task_id_targ_mean_sumed','device_name_task_id_targ_mean_sumed',\n",
    "         'device_size_task_id_targ_mean_sumed', 'career_task_id_targ_mean_sumed','gender_task_id_targ_mean_sumed',\n",
    "         'communication_onlinerate_task_id_targ_mean_sumed',\n",
    "       'communication_avgonline_30d_task_id_targ_mean_sumed',\n",
    "       'indu_name_task_id_targ_mean_sumed'\n",
    "         \n",
    "        ]\n",
    "PARAMS = {  'learning_rate':0.05, 'max_depth': -1,\n",
    "        'n_estimators': 200,\n",
    "        'num_leaves': 250,'objective': 'binary', 'n_jobs': -1,'boost_from_average': False}\n",
    "\n",
    "\n",
    "train_downded = negative_downsample(train_m[6277359:35897957],'label')\n",
    "train_data = lgb.Dataset(train_downded.drop('label',axis=1)[FEATS], label=train_downded.label, free_raw_data=True)\n",
    "aa = lgb.train(PARAMS,train_data)\n",
    "preds = aa.predict(train_m[35897957:].drop('label',axis=1)[FEATS])\n",
    "roc_auc_score(train_m[35897957:].label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8342143300039557"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATS = ['uid','task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','indu_name','age','uid_task_id_ptd','uid_adv_prim_id_ptd',\n",
    "        'uid_task_id_ptd','uid_adv_id_ptd_rng','uid_slot_id_ptd_rng','uid_spread_app_id_ptd_rng','his_app_size','app_first_class','gender',\n",
    "        'career','device_name','uid_targ_mean','task_id_targ_mean','adv_id_targ_mean','uid_targ_mean_sumed_x',\n",
    "        'task_id_targ_mean_sumed_x','uid_task_id_targ_mean_sumed','uid_adv_id_targ_mean_sumed','uid_targ_sum_sumed','task_id_targ_sum_sumed'\n",
    "         ,'career_age_gender_task_id_targ_mean_sumed', 'uid_ptd_rng','app_first_class','consume_purchase','up_membership_grade','up_life_duration',\n",
    "        'list_time','uid_cumcount','uid_task_id_cumcount','uid_spread_app_id_cumcount','uid_cumcount_task_id_targ_mean_sumed','age_task_id_targ_mean_sumed',\n",
    "         'city_task_id_targ_mean_sumed', 'city_rank_task_id_targ_mean_sumed','device_name_task_id_targ_mean_sumed',\n",
    "         'device_size_task_id_targ_mean_sumed', 'career_task_id_targ_mean_sumed','gender_task_id_targ_mean_sumed'\n",
    "         \n",
    "         \n",
    "        ]\n",
    "PARAMS = {  'learning_rate':0.05, 'max_depth': -1,\n",
    "        'n_estimators': 200,\n",
    "        'num_leaves': 250,'objective': 'binary', 'n_jobs': -1,'boost_from_average': False}\n",
    "\n",
    "\n",
    "train_downded = negative_downsample(train_m[6277359:35897957],'label')\n",
    "train_data = lgb.Dataset(train_downded.drop('label',axis=1)[FEATS], label=train_downded.label, free_raw_data=True)\n",
    "aa = lgb.train(PARAMS,train_data)\n",
    "preds = aa.predict(train_m[35897957:].drop('label',axis=1)[FEATS])\n",
    "roc_auc_score(train_m[35897957:].label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_downded = negative_downsample(train_m[6277359:],'label')\n",
    "train_data = lgb.Dataset(train_downded.drop('label',axis=1)[FEATS], label=train_downded.label, free_raw_data=True)\n",
    "aa = lgb.train(PARAMS,train_data)\n",
    "preds = aa.predict(test_m[FEATS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['probability']= preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id','probability']].set_index('id').to_csv('0.8303465728762043.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
